# 项目记忆文档

## 项目概述
医学PDF结构化信息提取系统，使用Qwen3-8B模型进行文档分类和信息提取。

## 当前版本: v7.6 (2026-01-12)

### 核心架构
1. **文档分类**: Qwen3-8B判断文档类型 (GUIDELINE/REVIEW/OTHER)
2. **混合页面选择**: 根据文档大小自适应选择不同策略
3. **动态Few-shot**: 根据类型加载对应的GPT-5.2预标注高质量示例
4. **结构化提取**: 提取元数据、推荐意见、关键发现、结论等

### v7.6 混合策略

```
PDF输入
    │
    ▼
获取总页数 total_pages
    │
    ├── ≤15页 (短文档)
    │   └── 全部保留，只跳过空白页
    │
    ├── 16-50页 (中等文档)
    │   └── 跳过目录/参考文献/空白页
    │
    └── >50页 (长文档)
        └── 智能选择最多50页
            - 前5页 + 最后3页
            - 高优先级章节(摘要/结论/推荐)
            - 按内容量填充剩余
```

### 性能指标
| 版本 | 成功率 | 评分 | ≥8分 | 耗时 | 长文档 |
|------|--------|------|------|------|--------|
| v7.2 | 97% | 9.60 | 100% | 58s | ❌ |
| v7.3 | 100% | 9.09 | 100% | 42s | ✅ |
| v7.6 | 待测 | 待测 | 待测 | 待测 | ✅ |

## 关键配置

### vLLM服务
- 脚本: `/root/autodl-tmp/vllm_server_v13.sh`
- 模型: Qwen3-8B fp16
- 端口: 8000
- API: `http://localhost:8000/v1/chat/completions`
- **重要**: 必须设置 `"chat_template_kwargs": {"enable_thinking": False}`

### GPT-5.2评估API
- URL: `https://api.bltcy.ai/v1/chat/completions`
- Model: `gpt-4.1`

### 文件路径
- 主提取器: `/root/pdf_summarization_deploy_20251225_093847/production_extractor_v7.py`
- Few-shot样本: `/root/pdf_summarization_deploy_20251225_093847/fewshot_samples/`
- PDF测试数据: `/root/autodl-tmp/pdf_input/pdf_input_09-1125/`
- 项目文档: `/root/autodl-tmp/docs/`

## 技术决策记录

### 为什么用混合策略?
- v7.2短文档评分9.60，但不支持长文档
- v7.3支持长文档，但短文档评分下降到9.09
- v7.6混合策略：短文档用v7.2逻辑，长文档用v7.3逻辑

### 混合策略代码实现
```python
if total_pages <= 15:      # 短文档
    # 全部保留，只跳过空白页
elif total_pages <= 50:    # 中等文档
    # 跳过目录/参考文献/空白页
else:                      # 长文档
    # 智能选择最多50页
```

### 为什么用fp16而不是int8?
- int8(GPTQ)在V100上速度慢14倍，成功率仅59%
- fp16性能稳定，推荐用于生产

### 为什么用动态few-shot?
- 静态few-shot(v6): 7.1分
- 动态few-shot(v7): 9.6分
- 不同文档类型结构差异大，需要针对性示例

## 版本历史

| 版本 | 日期 | 评分 | 关键改进 |
|------|------|------|----------|
| v7.2 | 01-08 | 9.60 | 动态few-shot，只支持前15页 |
| v7.3 | 01-12 | 9.09 | 智能页面选择，支持长文档 |
| v7.6 | 01-12 | 待测 | 混合策略，兼顾短长文档 |

## 已知限制
1. 长文档(>50页)只能选取部分页面，可能丢失信息
2. 复杂表格提取不完整
3. 部分PDF格式错误会导致失败(约3%)

## 下次继续的注意事项
1. vLLM服务需要先启动: `bash /root/autodl-tmp/vllm_server_v13.sh`
2. 等待模型加载完成(约2分钟)
3. 测试API: `curl http://localhost:8000/v1/models`
4. 使用 `production_extractor_v7.py` 进行提取
5. v7.6需要进行完整测试验证
