# 医学PDF提取系统 - 部署指南

## 1. 环境要求

### 硬件要求
- GPU: NVIDIA V100 16GB 或更高
- 内存: 32GB+
- 存储: 50GB+ (模型缓存)

### 软件要求
- Ubuntu 20.04+
- Python 3.8+
- CUDA 11.8+
- cuDNN 8.6+

## 2. 安装步骤

### 2.1 安装Python依赖
```bash
pip install vllm==0.13.0
pip install pymupdf
pip install requests
```

### 2.2 下载模型
```bash
# Qwen3-8B模型会在首次启动时自动下载
# 或手动下载到指定目录
huggingface-cli download Qwen/Qwen3-8B --local-dir /root/autodl-tmp/hf_cache/hub/models--Qwen--Qwen3-8B
```

### 2.3 配置文件结构
```
/root/pdf_summarization_deploy_20251225_093847/
├── production_extractor_v7.py   # 主程序
├── fewshot_samples/             # Few-shot示例 (必需)
│   ├── GUIDELINE_sample.json
│   ├── REVIEW_sample.json
│   └── OTHER_sample.json
└── vllm_server_v13.sh           # vLLM启动脚本
```

## 3. 启动服务

### 3.1 启动vLLM模型服务
```bash
cd /root/autodl-tmp
bash vllm_server_v13.sh
```

vLLM启动参数说明:
```bash
vllm serve Qwen/Qwen3-8B \
    --served-model-name qwen3-8b \
    --dtype float16 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.9 \
    --port 8000
```

### 3.2 验证服务状态
```bash
# 等待约2分钟后测试
curl http://localhost:8000/v1/models
```

预期输出:
```json
{"data": [{"id": "qwen3-8b", ...}]}
```

## 4. 使用方法

### 4.1 Python调用
```python
from production_extractor_v7 import extract_pdf

# 单文件提取
result = extract_pdf("/path/to/medical.pdf")

if result["success"]:
    print(f"类型: {result['doc_type']}")
    print(f"结果: {result['result']}")
else:
    print(f"错误: {result['error']}")
```

### 4.2 批量处理
```python
from production_extractor_v7 import MedicalPDFExtractor
from pathlib import Path

extractor = MedicalPDFExtractor()
pdf_dir = Path("/path/to/pdfs")

for pdf in pdf_dir.glob("*.pdf"):
    result = extractor.extract(str(pdf))
    # 处理结果...
```

### 4.3 作为API服务
```python
# api_server.py
from flask import Flask, request, jsonify
from production_extractor_v7 import extract_pdf

app = Flask(__name__)

@app.route('/extract', methods=['POST'])
def extract():
    pdf_path = request.json.get('pdf_path')
    result = extract_pdf(pdf_path)
    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

## 5. 配置说明

### 5.1 提取器配置
```python
# production_extractor_v7.py 中可配置项
LOCAL_API = "http://localhost:8000/v1/chat/completions"  # vLLM地址
max_pages = 15       # 最大处理页数
max_tokens = 6000    # LLM输出token限制
temperature = 0.2    # 生成温度
timeout = 300        # API超时秒数
```

### 5.2 关键参数
**必须设置**: 调用Qwen3-8B时需要禁用thinking模式
```python
"chat_template_kwargs": {"enable_thinking": False}
```

## 6. 性能优化

### 6.1 并发处理
```python
from concurrent.futures import ThreadPoolExecutor

def batch_extract(pdf_list, max_workers=2):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(extract_pdf, pdf_list))
    return results
```

推荐并发数:
- 用户上传: 2
- 批量处理: 5

### 6.2 内存优化
- GPU显存利用率: 0.9 (vLLM默认)
- 长文档截断: 前15页

## 7. 监控与日志

### 7.1 服务监控
```bash
# 检查vLLM进程
ps aux | grep vllm

# 检查GPU使用
nvidia-smi

# 检查API响应
curl http://localhost:8000/health
```

### 7.2 日志位置
- vLLM日志: 启动终端输出
- 提取日志: 可在代码中添加logging

## 8. 故障排除

### 8.1 常见问题

**Q: API返回 "max_tokens is too large"**
A: 减少输入文本长度或降低max_tokens值

**Q: JSON解析失败**
A: v7.2已修复大部分问题，若仍出现，检查PDF是否损坏

**Q: 提取结果不完整**
A: 长文档会被截断，这是已知限制

### 8.2 重启服务
```bash
# 停止vLLM
pkill -f vllm

# 重新启动
bash /root/autodl-tmp/vllm_server_v13.sh
```

## 9. 更新与维护

### 9.1 更新Few-shot样本
如需更新示例，修改 `fewshot_samples/` 下的JSON文件

### 9.2 版本回滚
保留历史版本在 `extractors_versioned/` 目录

## 10. 联系与支持

- 版本日志: `VERSION_LOG.md`
- 项目记忆: `memory.md`
- 测试结果: `v7_100pdf_test_results.json`
